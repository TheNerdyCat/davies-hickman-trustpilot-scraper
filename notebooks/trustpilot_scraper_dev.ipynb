{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25bc87e4-a8ec-4f2f-83e6-effed19e1234",
   "metadata": {},
   "source": [
    "# TrustPilot Scraper | Development\n",
    "\n",
    "This notebook conducts development and exploration into the TrustPilot web scraper, based on the specific parameters laid out from Davies Hickman. Further analysis of data is carried out in a separate notebook.\n",
    "\n",
    "*The key objective for the project is to identify companies on TrustPilot that are receiving negative reviews because of their poor customer service using WhatsApp, Messaging, SMS, Text, Webchat, etc.*\n",
    "\n",
    "Target Site: www.uk.trustpilot.com\n",
    "\n",
    "#### Categories:\n",
    " - Money & Insurance\n",
    " - Travel & Vacation\n",
    " - Food, Beverages & Tobacco\n",
    " - Restaurants & Bars\n",
    " - Events & Entertainment\n",
    " - Beauty & Well-being\n",
    " - Shopping & Fashion\n",
    " - Home & Garden\n",
    " - Vehicle & Transportation\n",
    " - Electronics & Technology\n",
    " - Animals & Pets\n",
    " - Business services (for logistics)\n",
    "\n",
    "### Scraper Usage\n",
    "The scraper outputs data files locally as it runs, so any interruptions do not ruin the output (and require starting again from scratch). If there are any interruptions to the loop, you can just look in the data directory to see what has already been scraped, comment out that category and rerun. \n",
    "\n",
    "There has been a restriction applied to the scraper in the form of maximum pages (for companies and reviews). The parameters in the function are `num_categories_pages` and `max_num_review_pages`. You can change these parameters to be larger numbers if you wish to get more data, although this will take more time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c427e0ba-d6c3-4cc4-8c11-f779cb07c5b6",
   "metadata": {},
   "source": [
    "## 0.0 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543de890-8e93-477e-8bd0-bc958e5ef9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation & stats\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from scipy.stats import truncnorm\n",
    "\n",
    "# Data visualisation\n",
    "import matplotlib.pyplot\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "# Standard libraries\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "# Web scraping\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef5a5bc-01b3-41ae-a28e-4dd8631f665d",
   "metadata": {},
   "source": [
    "## 1.0 Setup Config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d200fdba-3859-43c2-8691-8aac9ae0f0e9",
   "metadata": {},
   "source": [
    "### 1.01 Local paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3605ca33-75ee-4f22-afd7-c1055c146c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "notebooks_dir_path = os.getcwd()\n",
    "repo_dir_path = notebooks_dir_path.replace(\"/notebooks\", \"\")\n",
    "data_dir_path = os.path.join(repo_dir_path, \"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac21fcec-6101-4987-9194-4fc3b60e4ee4",
   "metadata": {},
   "source": [
    "### 1.02 URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898d5d3e-cd31-4a74-ac04-ca9788f50399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Category urls\n",
    "trustpilot_base_url = \"https://uk.trustpilot.com\"\n",
    "categories_base_url = os.path.join(trustpilot_base_url, \"categories\")\n",
    "\n",
    "# Url params\n",
    "# Page query\n",
    "num_categories_pages = 500\n",
    "categories_pages = range(1, num_categories_pages + 1)\n",
    "num_categories_pages_base_query = f\"?page={categories_pages[0]}\"\n",
    "\n",
    "# Other queries\n",
    "args_url = [\"sort=latest_review\"] # Sort by latest reviews\n",
    "args_url_query = f\"&{'&'.join(args_url)}\"\n",
    "\n",
    "print(\n",
    "    f\"Example complete URL: {os.path.join(categories_base_url, 'money_insurance') + num_categories_pages_base_query + args_url_query}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8d757e-107f-4260-911a-4263a628cab3",
   "metadata": {},
   "source": [
    "## 2.0 Scraper Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5028606-a730-45ac-8b26-24b8e9153c6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def random_wait_time(min_seconds, max_seconds, mean, std):\n",
    "    \"\"\"\n",
    "    Waits for a random number of seconds, following a truncated normal distribution.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    min_seconds : int or float\n",
    "        The minimum number of seconds to wait.\n",
    "    max_seconds : int or float\n",
    "        The maximum number of seconds to wait.\n",
    "    mean : float\n",
    "        The mean of the normal distribution.\n",
    "    std : float\n",
    "        The standard deviation of the normal distribution.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate the lower and upper bounds for truncation\n",
    "    a = (min_seconds - mean) / std\n",
    "    b = (max_seconds - mean) / std\n",
    "    random_wait_time = truncnorm.rvs(a, b, loc=mean, scale=std)\n",
    "    random_wait_time = max(min_seconds, min(max_seconds, random_wait_time))\n",
    "    time.sleep(random_wait_time)\n",
    "\n",
    "\n",
    "def scrape_trustpilot(trustpilot_base_url: str, \n",
    "                      categories_base_url: str, \n",
    "                      category_suffix: str, \n",
    "                      num_categories_pages: int, \n",
    "                      args_url_query: str,\n",
    "                      max_score: float,\n",
    "                      min_num_reviews: int,\n",
    "                      max_num_review_pages: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Scrapes company and review data from TrustPilot for a given category.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    trustpilot_base_url : str\n",
    "        The base URL of TrustPilot.\n",
    "    categories_base_url : str\n",
    "        The base URL for the category.\n",
    "    category_suffix : str\n",
    "        The category suffix to be appended to the categories_base_url.\n",
    "    num_categories_pages : int\n",
    "        The number of pages to scrape for companies within the category.\n",
    "    args_url_query : str\n",
    "        Additional query parameters to be added to the URL for category pages.\n",
    "    max_score : float\n",
    "        The maximum score a company can have to be included in the results.\n",
    "    min_num_reviews : int\n",
    "        The minimum number of reviews a company must have to be included in the results.\n",
    "    max_num_review_pages : int\n",
    "        The maximum number of review pages to scrape for each company.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A DataFrame containing company and review data for the specified category.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Create category dir if doesn't exist\n",
    "    data_category_dir_path = os.path.join(data_dir_path, category_suffix)\n",
    "    data_tmp_dir_path = os.path.join(data_category_dir_path, \"tmp\")\n",
    "    data_tmp_companies_dir_path = os.path.join(data_tmp_dir_path, \"companies\")\n",
    "    data_tmp_reviews_dir_path = os.path.join(data_tmp_dir_path, \"reviews\")\n",
    "    \n",
    "    if not os.path.exists(data_category_dir_path):\n",
    "        os.makedirs(data_category_dir_path)\n",
    "    if not os.path.exists(data_tmp_dir_path):\n",
    "        os.makedirs(data_tmp_dir_path)\n",
    "    if not os.path.exists(data_tmp_companies_dir_path):\n",
    "        os.makedirs(data_tmp_companies_dir_path)\n",
    "    if not os.path.exists(data_tmp_reviews_dir_path):\n",
    "        os.makedirs(data_tmp_reviews_dir_path)\n",
    "\n",
    "    category_target_url = os.path.join(categories_base_url, category_suffix)\n",
    "\n",
    "    # Get categories page list\n",
    "    categories_pages = range(1, min(num_categories_pages, 500) + 1) # Always a max of 500\n",
    "\n",
    "    # Initialise dataframe of companies\n",
    "    companies_df = pd.DataFrame()\n",
    "\n",
    "    # Loop through category/company pages on TrustPilot\n",
    "    print(f\"Collecting company data for {category_suffix}...\")\n",
    "    for categories_page in tqdm(categories_pages):\n",
    "        # Get page query\n",
    "        page_query = f\"?page={categories_page}\"\n",
    "        # Create complete url to request\n",
    "        category_target_complete_url = f\"{category_target_url}{page_query}{args_url_query}\"\n",
    "        response_companies = requests.get(category_target_complete_url)\n",
    "\n",
    "        # If response code is OK, scrape html\n",
    "        if response_companies.status_code == 200:\n",
    "            soup_companies = BeautifulSoup(response_companies.text, 'html.parser')\n",
    "\n",
    "            # Extract company names\n",
    "            company_names = soup_companies.find_all(\n",
    "                \"p\", \n",
    "                class_=\"typography_heading-xs__jSwUz typography_appearance-default__AAY17 styles_displayName__GOhL2\"\n",
    "            )\n",
    "            company_names = [element.text for element in company_names]\n",
    "            # Extract links to reviews\n",
    "            review_links = soup_companies.find_all(\n",
    "                \"a\", \n",
    "                class_=\"link_internal__7XN06 link_wrapper__5ZJEx styles_linkWrapper__UWs5j\"\n",
    "            )\n",
    "            review_links = [os.path.join(trustpilot_base_url, element.get(\"href\")[1:]) for element in review_links]\n",
    "            # Extract ratings\n",
    "            company_scores_num_reviews = soup_companies.find_all(\n",
    "                \"p\", \n",
    "                class_=\"typography_body-m__xgxZ_ typography_appearance-subtle__8_H2l styles_ratingText__yQ5S7\"\n",
    "            )\n",
    "            company_scores = [\n",
    "                float(re.search(r\"(\\d+\\.\\d+)\", element.text.split(\"|\")[0]).group(1)) for element in company_scores_num_reviews\n",
    "            ]\n",
    "            # Extract num reviews\n",
    "            num_reviews = [int(\"\".join(filter(str.isdigit, element.text.split(\"|\")[1]))) for element in company_scores_num_reviews]\n",
    "    \n",
    "            # Create dataframe of company names and links for this batch\n",
    "            try:\n",
    "                companies_df_temp = pd.DataFrame(\n",
    "                    {\n",
    "                        \"company_name\": company_names,\n",
    "                        \"review_link\": review_links,\n",
    "                        \"company_score\": company_scores,\n",
    "                        \"num_reviews\": num_reviews,\n",
    "                    }\n",
    "                )\n",
    "                companies_df_temp[\"categories_page\"] = categories_page\n",
    "                # Write unfiltered tmp file to memory\n",
    "                companies_df_temp.to_csv(\n",
    "                    os.path.join(data_tmp_companies_dir_path, f\"companies_df_{category_suffix}_{categories_page}_tmp.csv\"), \n",
    "                    index=False\n",
    "                )\n",
    "                # Filter to parameters set\n",
    "                companies_df_temp = companies_df_temp.loc[\n",
    "                    (companies_df_temp.company_score <= max_score) &\n",
    "                    (companies_df_temp.num_reviews >= min_num_reviews)\n",
    "                ]\n",
    "                # Concatenate sub df with master df\n",
    "                companies_df = pd.concat(\n",
    "                    [companies_df, companies_df_temp],\n",
    "                    ignore_index=True\n",
    "                )\n",
    "                # Write master df to memory - overwrite each iteration\n",
    "                companies_df.to_csv(\n",
    "                    os.path.join(data_category_dir_path, f\"companies_df_{category_suffix}_raw.csv\"),\n",
    "                    index=False\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Error creating companies_df_temp for page {page_query}: {e}\")\n",
    "                pass\n",
    "\n",
    "        else:\n",
    "            companies_df = pd.DataFrame()\n",
    "            print(f\"Failed to fetch response_companies page. Status code: {response_companies.status_code}\")\n",
    "\n",
    "        random_wait_time(min_seconds=2, max_seconds=12, mean=3.5, std=0.75) # Random wait between looping through category companies\n",
    "        \n",
    "    # Add dummy columns for address\n",
    "    companies_df[\"address\"] = None\n",
    "    companies_df[\"is_uk\"] = None\n",
    "    # Rewrite final companies df to memory\n",
    "    companies_df = companies_df.drop_duplicates(\n",
    "        subset=[\"company_name\", \"review_link\", \"company_score\"]\n",
    "    )\n",
    "    companies_df.to_csv(\n",
    "        os.path.join(data_category_dir_path, f\"companies_df_{category_suffix}_raw.csv\"),\n",
    "        index=False\n",
    "    )\n",
    "    \n",
    "    print(\"Company data collected.\")\n",
    "    print(f\"Collecting review data for {category_suffix}\")\n",
    "    companies_df_full = pd.DataFrame()\n",
    "\n",
    "    # Loop through scraped companies to get reviews\n",
    "    for idx, row in tqdm(companies_df.iterrows(), total=companies_df.shape[0]):\n",
    "        company_name = row[\"company_name\"]\n",
    "        company_reviews_base_url = row[\"review_link\"]\n",
    "        \n",
    "        response_reviews = requests.get(company_reviews_base_url)\n",
    "    \n",
    "        # If response code is OK, scrape html\n",
    "        if response_reviews.status_code == 200:\n",
    "            soup_reviews = BeautifulSoup(response_reviews.text, \"html.parser\")\n",
    "        \n",
    "            try:\n",
    "                # Get company address information\n",
    "                address_list = soup_reviews.find(\n",
    "                    \"ul\", class_=\"typography_body-m__xgxZ_ typography_appearance-default__AAY17 styles_contactInfoAddressList__RxiJI\"\n",
    "                )\n",
    "                address_list = [element.text.lower() for element in address_list]\n",
    "                address_concat = \", \".join(address_list) # Concatenate to one string\n",
    "                # Update df\n",
    "                companies_df.loc[idx, \"address\"] = address_concat\n",
    "                if \"united kingdom\" in address_concat or \"uk\" in address_concat:\n",
    "                    companies_df.loc[idx, \"is_uk\"] = True\n",
    "                else:\n",
    "                    companies_df.loc[idx, \"is_uk\"] = False\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to get address data for {company_name}: {e}\")\n",
    "                pass\n",
    "\n",
    "            # Extract number of review pages\n",
    "            review_pages = soup_reviews.find(\n",
    "                \"nav\", class_=\"pagination_pagination___F1qS\"\n",
    "            )\n",
    "\n",
    "            review_pages = [element.text for element in review_pages]\n",
    "            num_review_pages = int(review_pages[len(review_pages) - 2])\n",
    "            # Define maximum number of review pages to loop through\n",
    "            num_review_pages = min(num_review_pages, max_num_review_pages + 1)\n",
    "            review_pages = range(1, num_review_pages + 1)\n",
    "\n",
    "            # Initialise dataframe of reviews\n",
    "            reviews_df = pd.DataFrame()\n",
    "\n",
    "            # Loop through reviews and review pages\n",
    "            for review_page in review_pages:\n",
    "                company_reviews_page_url = f\"{company_reviews_base_url}?page={review_page}\"\n",
    "                \n",
    "                response_reviews_page = requests.get(company_reviews_page_url)\n",
    "            \n",
    "                # If response code is OK, scrape html\n",
    "                if response_reviews_page.status_code == 200:\n",
    "                    soup_reviews_page = BeautifulSoup(response_reviews_page.text, \"html.parser\")\n",
    "\n",
    "                    try:\n",
    "                        # Get review/experience dates\n",
    "                        reviews_dates = soup_reviews_page.find_all(\n",
    "                            \"p\", class_=\"typography_body-m__xgxZ_ typography_appearance-default__AAY17\"\n",
    "                        )    \n",
    "                        # Clean dates\n",
    "                        dates = []\n",
    "                        for date_element in reviews_dates:\n",
    "                            date_text = date_element.get_text(strip=True)\n",
    "                            date_value = date_text.split(\":\")[-1].strip()\n",
    "                            # Convert the date to a different format\n",
    "                            try:\n",
    "                                # Assuming the date is in the format \"21 November 2023\"\n",
    "                                datetime_object = datetime.datetime.strptime(date_value, \"%d %B %Y\")\n",
    "                                formatted_date = datetime_object.strftime(\"%Y-%m-%d\")\n",
    "                                dates.append(formatted_date)\n",
    "                            except Exception as e:\n",
    "                                pass # Pass on non-date elements\n",
    "\n",
    "                        # Get review score\n",
    "                        reviews_scores = soup_reviews_page.find_all(\"div\", class_=\"styles_reviewHeader__iU9Px\")\n",
    "                        # Clean scores\n",
    "                        scores = []\n",
    "                        for score in reviews_scores:\n",
    "                            try:\n",
    "                                score = score[\"data-service-review-rating\"]\n",
    "                                score = int(score)\n",
    "                                scores.append(score)\n",
    "                            except Exception as e:\n",
    "                                pass\n",
    "\n",
    "                        # Get review text\n",
    "                        reviews_reviews = soup_reviews_page.find_all(\n",
    "                            'p', \n",
    "                            class_='typography_body-l__KUYFJ typography_appearance-default__AAY17 typography_color-black__5LYEn'\n",
    "                        )\n",
    "                        reviews = [element.text for element in reviews_reviews]\n",
    "\n",
    "                        try:\n",
    "                            # Create dataframe of reviews for this batch\n",
    "                            reviews_df_temp = pd.DataFrame(\n",
    "                                {\n",
    "                                    \"date\": dates,\n",
    "                                    \"score\": scores,\n",
    "                                    \"review\": reviews,\n",
    "                                }\n",
    "                            )\n",
    "                            reviews_df_temp[\"reviews_page\"] = review_page\n",
    "                            reviews_df_temp[\"company_name\"] = company_name                        \n",
    "                            # Write unfiltered tmp file to memory\n",
    "                            reviews_df_temp.to_csv(\n",
    "                                os.path.join(\n",
    "                                    data_tmp_reviews_dir_path, f\"reviews_df_{category_suffix}_{company_name}_{review_page}_tmp.csv\"\n",
    "                                ), \n",
    "                                index=False\n",
    "                            )\n",
    "                            # Concatenate sub df with master df\n",
    "                            reviews_df = pd.concat(\n",
    "                                [reviews_df, reviews_df_temp],\n",
    "                                ignore_index=True\n",
    "                            )\n",
    "                            # Write master df to memory - overwrite each iteration\n",
    "                            reviews_df.to_csv(\n",
    "                                os.path.join(data_category_dir_path, f\"reviews_df_{category_suffix}_{company_name}.csv\"),\n",
    "                                index=False\n",
    "                            )\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error creating reviews_df_temp for {company_name}/{review_page}: {e}\")\n",
    "                            pass\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        print(f\"Failed to extract review data for {company_name}/{review_page}: {e}\")\n",
    "                        pass\n",
    "                        \n",
    "                random_wait_time(min_seconds=2, max_seconds=12, mean=3.5, std=0.75) # Random wait through individual review pages\n",
    "\n",
    "            # Merge reviews_df with companies_df\n",
    "            companies_df_full = pd.concat(\n",
    "                [\n",
    "                    companies_df_full,\n",
    "                    pd.merge(companies_df, reviews_df, on=\"company_name\", how=\"inner\")\n",
    "                ],\n",
    "                ignore_index=True\n",
    "            )\n",
    "            companies_df_full.to_csv(os.path.join(data_category_dir_path, f\"companies_df_{category_suffix}_full.csv\"), index=False)\n",
    "        else:\n",
    "            print(f\"Failed to fetch response_reviews page. Status code: {response_reviews.status_code}\")\n",
    "            pass\n",
    "        \n",
    "        random_wait_time(min_seconds=2, max_seconds=12, mean=3.5, std=0.75) # Random wait through getting company review pages\n",
    "        \n",
    "\n",
    "    return companies_df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cf02bf-cc5f-4d1d-9d7d-ac5abd904c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run scraper for all categories\n",
    "category_suffix_list = [\n",
    "    \"money_insurance\",\n",
    "    \"travel_vacation\",\n",
    "    \"food_beverages_tobacco\",\n",
    "    \"restaurants_bars\",\n",
    "    \"events_entertainment\",\n",
    "    \"beauty_wellbeing\",\n",
    "    \"shopping_fashion\",\n",
    "    \"home_garden\",\n",
    "    \"vehicles_transportation\",\n",
    "    \"electronics_technology\",\n",
    "    \"animals_pets\",\n",
    "    \"shipping_logistics\"\n",
    "]\n",
    "\n",
    "for category_suffix in category_suffix_list:\n",
    "    companies_df_full = scrape_trustpilot(\n",
    "        trustpilot_base_url=trustpilot_base_url, \n",
    "        categories_base_url=categories_base_url, \n",
    "        category_suffix=category_suffix,\n",
    "        num_categories_pages=100,\n",
    "        args_url_query=args_url_query,\n",
    "        max_score=3.0,\n",
    "        min_num_reviews=1000,\n",
    "        max_num_review_pages=50\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2352fe78-5d70-4af1-bf91-e8b7e7ae4230",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d3cbac-6e9d-44f2-b7db-66aaa597d1db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a081eefa-680b-480b-9e11-30cd97c6925f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24ebcf6-5c36-48cd-ab35-2c36a49ed49d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a87412e-e2b3-4158-9894-3fca7c102831",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0707272-6352-419f-9868-d47e8ff19a22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b996f6-c533-43c4-80f9-2c2d0c39d25e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464e4ec7-39cd-4aba-a6b2-4bcfd46ae12d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4d62fb-1318-4e60-a1af-7d4de6a2ebb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2203f3be-dd54-4a31-9bdb-263570664f62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "davies-hickman-trustpilot-scraper",
   "language": "python",
   "name": "davies-hickman-trustpilot-scraper"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
