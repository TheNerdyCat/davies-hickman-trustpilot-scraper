{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25bc87e4-a8ec-4f2f-83e6-effed19e1234",
   "metadata": {},
   "source": [
    "# TrustPilot Scraper | Development\n",
    "\n",
    "This notebook conducts development and exploration into the TrustPilot web scraper, based on the specific parameters laid out from Davies Hickman. Further analysis of data is carried out in a separate notebook.\n",
    "\n",
    "*The key objective for the project is to identify companies on TrustPilot that are receiving negative reviews because of their poor customer service using WhatsApp, Messaging, SMS, Text, Webchat, etc.*\n",
    "\n",
    "Target Site: www.uk.trustpilot.com\n",
    "\n",
    "#### Categories:\n",
    " - Money & Insurance\n",
    " - Travel & Vacation\n",
    " - Food, Beverages & Tobacco\n",
    " - Restaurants & Bars\n",
    " - Events & Entertainment\n",
    " - Beauty & Well-being\n",
    " - Shopping & Fashion\n",
    " - Home & Garden\n",
    " - Vehicle & Transportation\n",
    " - Electronics & Technology\n",
    " - Animals & Pets\n",
    " - Business services (for logistics)\n",
    "\n",
    "#### Challenges:\n",
    " - Can't filter immediately to below 3 stars - only have Any, 3+, 4+ or 4.5+. This means we have to go through the whole of the TrustPilot category and find the companies with 3 stars or less."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c427e0ba-d6c3-4cc4-8c11-f779cb07c5b6",
   "metadata": {},
   "source": [
    "## 0.0 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "543de890-8e93-477e-8bd0-bc958e5ef9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation & stats\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from scipy.stats import truncnorm\n",
    "\n",
    "# Data visualisation\n",
    "import matplotlib.pyplot\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "# Standard libraries\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "# Web scraping\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef5a5bc-01b3-41ae-a28e-4dd8631f665d",
   "metadata": {},
   "source": [
    "## 1.0 Setup Config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d200fdba-3859-43c2-8691-8aac9ae0f0e9",
   "metadata": {},
   "source": [
    "### 1.01 Local paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3605ca33-75ee-4f22-afd7-c1055c146c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "notebooks_dir_path = os.getcwd()\n",
    "repo_dir_path = notebooks_dir_path.replace(\"/notebooks\", \"\")\n",
    "data_dir_path = os.path.join(repo_dir_path, \"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac21fcec-6101-4987-9194-4fc3b60e4ee4",
   "metadata": {},
   "source": [
    "### 1.02 URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "898d5d3e-cd31-4a74-ac04-ca9788f50399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example complete URL: https://uk.trustpilot.com/categories/money_insurance?page=1&sort=latest_review\n"
     ]
    }
   ],
   "source": [
    "# Category urls\n",
    "trustpilot_base_url = \"https://uk.trustpilot.com\"\n",
    "categories_base_url = os.path.join(trustpilot_base_url, \"categories\")\n",
    "\n",
    "# Url params\n",
    "# Page query\n",
    "num_categories_pages = 500\n",
    "categories_pages = range(1, num_categories_pages + 1)\n",
    "num_categories_pages_base_query = f\"?page={categories_pages[0]}\"\n",
    "\n",
    "# Other queries\n",
    "args_url = [\"sort=latest_review\"] # Sort by latest reviews\n",
    "args_url_query = f\"&{'&'.join(args_url)}\"\n",
    "\n",
    "print(\n",
    "    f\"Example complete URL: {os.path.join(categories_base_url, 'money_insurance') + num_categories_pages_base_query + args_url_query}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8d757e-107f-4260-911a-4263a628cab3",
   "metadata": {},
   "source": [
    "## 2.0 Scraper Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5028606-a730-45ac-8b26-24b8e9153c6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def random_wait_time(min_seconds, max_seconds, mean, std):\n",
    "    \"\"\"\n",
    "    Waits for a random number of seconds, following a truncated normal distribution.\n",
    "    \"\"\"\n",
    "    # Calculate the lower and upper bounds for truncation\n",
    "    a = (min_seconds - mean) / std\n",
    "    b = (max_seconds - mean) / std\n",
    "    random_wait_time = truncnorm.rvs(a, b, loc=mean, scale=std)\n",
    "    random_wait_time = max(min_seconds, min(max_seconds, random_wait_time))\n",
    "    time.sleep(random_wait_time)\n",
    "\n",
    "\n",
    "def scrape_trustpilot(trustpilot_base_url: str, \n",
    "                      categories_base_url: str, \n",
    "                      category_suffix: str, \n",
    "                      num_categories_pages: int, \n",
    "                      args_url_query: str,\n",
    "                      max_score: float,\n",
    "                      min_num_reviews: int,\n",
    "                      max_num_review_pages: int) -> pd.DataFrame:\n",
    "\n",
    "\n",
    "    # Create category dir if doesn't exist\n",
    "    data_category_dir_path = os.path.join(data_dir_path, category_suffix)\n",
    "    data_tmp_dir_path = os.path.join(data_category_dir_path, \"tmp\")\n",
    "    data_tmp_companies_dir_path = os.path.join(data_tmp_dir_path, \"companies\")\n",
    "    data_tmp_reviews_dir_path = os.path.join(data_tmp_dir_path, \"reviews\")\n",
    "    \n",
    "    if not os.path.exists(data_category_dir_path):\n",
    "        os.makedirs(data_category_dir_path)\n",
    "    if not os.path.exists(data_tmp_dir_path):\n",
    "        os.makedirs(data_tmp_dir_path)\n",
    "    if not os.path.exists(data_tmp_companies_dir_path):\n",
    "        os.makedirs(data_tmp_companies_dir_path)\n",
    "    if not os.path.exists(data_tmp_reviews_dir_path):\n",
    "        os.makedirs(data_tmp_reviews_dir_path)\n",
    "\n",
    "    category_target_url = os.path.join(categories_base_url, category_suffix)\n",
    "\n",
    "    # Get categories page list\n",
    "    categories_pages = range(1, min(num_categories_pages, 500) + 1) # Always a max of 500\n",
    "\n",
    "    # Initialise dataframe of companies\n",
    "    companies_df = pd.DataFrame()\n",
    "\n",
    "    # Loop through category/company pages on TrustPilot\n",
    "    print(f\"Collecting company data for {category_suffix}...\")\n",
    "    for categories_page in tqdm(categories_pages):\n",
    "        # Get page query\n",
    "        page_query = f\"?page={categories_page}\"\n",
    "        # Create complete url to request\n",
    "        category_target_complete_url = f\"{category_target_url}{page_query}{args_url_query}\"\n",
    "        response_companies = requests.get(category_target_complete_url)\n",
    "\n",
    "        # If response code is OK, scrape html\n",
    "        if response_companies.status_code == 200:\n",
    "            soup_companies = BeautifulSoup(response_companies.text, 'html.parser')\n",
    "\n",
    "            # Extract company names\n",
    "            company_names = soup_companies.find_all(\n",
    "                \"p\", \n",
    "                class_=\"typography_heading-xs__jSwUz typography_appearance-default__AAY17 styles_displayName__GOhL2\"\n",
    "            )\n",
    "            company_names = [element.text for element in company_names]\n",
    "            # Extract links to reviews\n",
    "            review_links = soup_companies.find_all(\n",
    "                \"a\", \n",
    "                class_=\"link_internal__7XN06 link_wrapper__5ZJEx styles_linkWrapper__UWs5j\"\n",
    "            )\n",
    "            review_links = [os.path.join(trustpilot_base_url, element.get(\"href\")[1:]) for element in review_links]\n",
    "            # Extract ratings\n",
    "            company_scores_num_reviews = soup_companies.find_all(\n",
    "                \"p\", \n",
    "                class_=\"typography_body-m__xgxZ_ typography_appearance-subtle__8_H2l styles_ratingText__yQ5S7\"\n",
    "            )\n",
    "            company_scores = [\n",
    "                float(re.search(r\"(\\d+\\.\\d+)\", element.text.split(\"|\")[0]).group(1)) for element in company_scores_num_reviews\n",
    "            ]\n",
    "            # Extract num reviews\n",
    "            num_reviews = [int(\"\".join(filter(str.isdigit, element.text.split(\"|\")[1]))) for element in company_scores_num_reviews]\n",
    "    \n",
    "            # Create dataframe of company names and links for this batch\n",
    "            try:\n",
    "                companies_df_temp = pd.DataFrame(\n",
    "                    {\n",
    "                        \"company_name\": company_names,\n",
    "                        \"review_link\": review_links,\n",
    "                        \"company_score\": company_scores,\n",
    "                        \"num_reviews\": num_reviews,\n",
    "                    }\n",
    "                )\n",
    "                companies_df_temp[\"categories_page\"] = categories_page\n",
    "                # Write unfiltered tmp file to memory\n",
    "                companies_df_temp.to_csv(\n",
    "                    os.path.join(data_tmp_companies_dir_path, f\"companies_df_{category_suffix}_{categories_page}_tmp.csv\"), \n",
    "                    index=False\n",
    "                )\n",
    "                # Filter to parameters set\n",
    "                companies_df_temp = companies_df_temp.loc[\n",
    "                    (companies_df_temp.company_score <= max_score) &\n",
    "                    (companies_df_temp.num_reviews >= min_num_reviews)\n",
    "                ]\n",
    "                # Concatenate sub df with master df\n",
    "                companies_df = pd.concat(\n",
    "                    [companies_df, companies_df_temp],\n",
    "                    ignore_index=True\n",
    "                )\n",
    "                # Write master df to memory - overwrite each iteration\n",
    "                companies_df.to_csv(\n",
    "                    os.path.join(data_category_dir_path, f\"companies_df_{category_suffix}_raw.csv\"),\n",
    "                    index=False\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Error creating companies_df_temp for page {page_query}: {e}\")\n",
    "                pass\n",
    "\n",
    "        else:\n",
    "            companies_df = pd.DataFrame()\n",
    "            print(f\"Failed to fetch response_companies page. Status code: {response_companies.status_code}\")\n",
    "\n",
    "        random_wait_time(min_seconds=2, max_seconds=12, mean=3.5, std=0.75) # Random wait between looping through category companies\n",
    "        \n",
    "    # Add dummy columns for address\n",
    "    companies_df[\"address\"] = None\n",
    "    companies_df[\"is_uk\"] = None\n",
    "    # Rewrite final companies df to memory\n",
    "    companies_df = companies_df.drop_duplicates(\n",
    "        subset=[\"company_name\", \"review_link\", \"company_score\"]\n",
    "    )\n",
    "    companies_df.to_csv(\n",
    "        os.path.join(data_category_dir_path, f\"companies_df_{category_suffix}_raw.csv\"),\n",
    "        index=False\n",
    "    )\n",
    "    \n",
    "    print(\"Company data collected.\")\n",
    "    print(f\"Collecting review data for {category_suffix}\")\n",
    "    companies_df_full = pd.DataFrame()\n",
    "\n",
    "    # Loop through scraped companies to get reviews\n",
    "    for idx, row in tqdm(companies_df.iterrows(), total=companies_df.shape[0]):\n",
    "        company_name = row[\"company_name\"]\n",
    "        company_reviews_base_url = row[\"review_link\"]\n",
    "        \n",
    "        response_reviews = requests.get(company_reviews_base_url)\n",
    "    \n",
    "        # If response code is OK, scrape html\n",
    "        if response_reviews.status_code == 200:\n",
    "            soup_reviews = BeautifulSoup(response_reviews.text, \"html.parser\")\n",
    "        \n",
    "            try:\n",
    "                # Get company address information\n",
    "                address_list = soup_reviews.find(\n",
    "                    \"ul\", class_=\"typography_body-m__xgxZ_ typography_appearance-default__AAY17 styles_contactInfoAddressList__RxiJI\"\n",
    "                )\n",
    "                address_list = [element.text.lower() for element in address_list]\n",
    "                address_concat = \", \".join(address_list) # Concatenate to one string\n",
    "                # Update df\n",
    "                companies_df.loc[idx, \"address\"] = address_concat\n",
    "                if \"united kingdom\" in address_concat or \"uk\" in address_concat:\n",
    "                    companies_df.loc[idx, \"is_uk\"] = True\n",
    "                else:\n",
    "                    companies_df.loc[idx, \"is_uk\"] = False\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to get address data for {company_name}: {e}\")\n",
    "                pass\n",
    "\n",
    "            # Extract number of review pages\n",
    "            review_pages = soup_reviews.find(\n",
    "                \"nav\", class_=\"pagination_pagination___F1qS\"\n",
    "            )\n",
    "\n",
    "            review_pages = [element.text for element in review_pages]\n",
    "            num_review_pages = int(review_pages[len(review_pages) - 2])\n",
    "            # Define maximum number of review pages to loop through\n",
    "            num_review_pages = min(num_review_pages, max_num_review_pages + 1)\n",
    "            review_pages = range(1, num_review_pages + 1)\n",
    "\n",
    "            # Initialise dataframe of reviews\n",
    "            reviews_df = pd.DataFrame()\n",
    "\n",
    "            # Loop through reviews and review pages\n",
    "            for review_page in review_pages:\n",
    "                company_reviews_page_url = f\"{company_reviews_base_url}?page={review_page}\"\n",
    "                \n",
    "                response_reviews_page = requests.get(company_reviews_page_url)\n",
    "            \n",
    "                # If response code is OK, scrape html\n",
    "                if response_reviews_page.status_code == 200:\n",
    "                    soup_reviews_page = BeautifulSoup(response_reviews_page.text, \"html.parser\")\n",
    "\n",
    "                    try:\n",
    "                        # Get review/experience dates\n",
    "                        reviews_dates = soup_reviews_page.find_all(\n",
    "                            \"p\", class_=\"typography_body-m__xgxZ_ typography_appearance-default__AAY17\"\n",
    "                        )    \n",
    "                        # Clean dates\n",
    "                        dates = []\n",
    "                        for date_element in reviews_dates:\n",
    "                            date_text = date_element.get_text(strip=True)\n",
    "                            date_value = date_text.split(\":\")[-1].strip()\n",
    "                            # Convert the date to a different format\n",
    "                            try:\n",
    "                                # Assuming the date is in the format \"21 November 2023\"\n",
    "                                datetime_object = datetime.datetime.strptime(date_value, \"%d %B %Y\")\n",
    "                                formatted_date = datetime_object.strftime(\"%Y-%m-%d\")\n",
    "                                dates.append(formatted_date)\n",
    "                            except Exception as e:\n",
    "                                pass # Pass on non-date elements\n",
    "\n",
    "                        # Get review score\n",
    "                        reviews_scores = soup_reviews_page.find_all(\"div\", class_=\"styles_reviewHeader__iU9Px\")\n",
    "                        # Clean scores\n",
    "                        scores = []\n",
    "                        for score in reviews_scores:\n",
    "                            try:\n",
    "                                score = score[\"data-service-review-rating\"]\n",
    "                                score = int(score)\n",
    "                                scores.append(score)\n",
    "                            except Exception as e:\n",
    "                                pass\n",
    "\n",
    "                        # Get review text\n",
    "                        reviews_reviews = soup_reviews_page.find_all(\n",
    "                            'p', \n",
    "                            class_='typography_body-l__KUYFJ typography_appearance-default__AAY17 typography_color-black__5LYEn'\n",
    "                        )\n",
    "                        reviews = [element.text for element in reviews_reviews]\n",
    "\n",
    "                        try:\n",
    "                            # Create dataframe of reviews for this batch\n",
    "                            reviews_df_temp = pd.DataFrame(\n",
    "                                {\n",
    "                                    \"date\": dates,\n",
    "                                    \"score\": scores,\n",
    "                                    \"review\": reviews,\n",
    "                                }\n",
    "                            )\n",
    "                            reviews_df_temp[\"reviews_page\"] = review_page\n",
    "                            reviews_df_temp[\"company_name\"] = company_name                        \n",
    "                            # Write unfiltered tmp file to memory\n",
    "                            reviews_df_temp.to_csv(\n",
    "                                os.path.join(\n",
    "                                    data_tmp_reviews_dir_path, f\"reviews_df_{category_suffix}_{company_name}_{review_page}_tmp.csv\"\n",
    "                                ), \n",
    "                                index=False\n",
    "                            )\n",
    "                            # Concatenate sub df with master df\n",
    "                            reviews_df = pd.concat(\n",
    "                                [reviews_df, reviews_df_temp],\n",
    "                                ignore_index=True\n",
    "                            )\n",
    "                            # Write master df to memory - overwrite each iteration\n",
    "                            reviews_df.to_csv(\n",
    "                                os.path.join(data_category_dir_path, f\"reviews_df_{category_suffix}_{company_name}.csv\"),\n",
    "                                index=False\n",
    "                            )\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error creating reviews_df_temp for {company_name}/{review_page}: {e}\")\n",
    "                            pass\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        print(f\"Failed to extract review data for {company_name}/{review_page}: {e}\")\n",
    "                        pass\n",
    "                        \n",
    "                random_wait_time(min_seconds=2, max_seconds=12, mean=3.5, std=0.75) # Random wait through individual review pages\n",
    "\n",
    "            # Merge reviews_df with companies_df\n",
    "            companies_df_full = pd.concat(\n",
    "                [\n",
    "                    companies_df_full,\n",
    "                    pd.merge(companies_df, reviews_df, on=\"company_name\", how=\"inner\")\n",
    "                ],\n",
    "                ignore_index=True\n",
    "            )\n",
    "            companies_df_full.to_csv(os.path.join(data_category_dir_path, f\"companies_df_{category_suffix}_full.csv\"), index=False)\n",
    "        else:\n",
    "            print(f\"Failed to fetch response_reviews page. Status code: {response_reviews.status_code}\")\n",
    "            pass\n",
    "        \n",
    "        random_wait_time(min_seconds=2, max_seconds=12, mean=3.5, std=0.75) # Random wait through getting company review pages\n",
    "        \n",
    "\n",
    "    return companies_df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3cf02bf-cc5f-4d1d-9d7d-ac5abd904c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting company data for shipping_logistics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███████████████████████████████████████████████▉                                                                           | 39/100 [02:26<03:38,  3.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error creating companies_df_temp for page ?page=40: All arrays must be of the same length\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|█████████████████████████████████████████████████████████████████████████████████████████▊                                 | 73/100 [04:43<01:39,  3.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error creating companies_df_temp for page ?page=74: All arrays must be of the same length\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████████████████████████████████████████████████████████████████████████████████████████                                | 74/100 [04:47<01:34,  3.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error creating companies_df_temp for page ?page=75: All arrays must be of the same length\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|████████████████████████████████████████████████████████████████████████████████████████████▎                              | 75/100 [04:51<01:30,  3.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error creating companies_df_temp for page ?page=76: All arrays must be of the same length\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|█████████████████████████████████████████████████████████████████████████████████████████████▍                             | 76/100 [04:55<01:36,  4.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error creating companies_df_temp for page ?page=77: All arrays must be of the same length\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|██████████████████████████████████████████████████████████████████████████████████████████████▋                            | 77/100 [04:59<01:31,  3.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error creating companies_df_temp for page ?page=78: All arrays must be of the same length\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████████████████████████████████████████████████████████████████████████████████████████████▉                           | 78/100 [05:02<01:20,  3.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error creating companies_df_temp for page ?page=79: All arrays must be of the same length\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|█████████████████████████████████████████████████████████████████████████████████████████████████▏                         | 79/100 [05:05<01:13,  3.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error creating companies_df_temp for page ?page=80: All arrays must be of the same length\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|██████████████████████████████████████████████████████████████████████████████████████████████████▍                        | 80/100 [05:09<01:09,  3.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error creating companies_df_temp for page ?page=81: All arrays must be of the same length\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|███████████████████████████████████████████████████████████████████████████████████████████████████▋                       | 81/100 [05:12<01:06,  3.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error creating companies_df_temp for page ?page=82: All arrays must be of the same length\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████████████████████████████████████████████████████████████████████████████████████████████████▊                      | 82/100 [05:17<01:06,  3.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error creating companies_df_temp for page ?page=83: All arrays must be of the same length\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|██████████████████████████████████████████████████████████████████████████████████████████████████████                     | 83/100 [05:21<01:08,  4.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error creating companies_df_temp for page ?page=84: All arrays must be of the same length\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|███████████████████████████████████████████████████████████████████████████████████████████████████████▎                   | 84/100 [05:26<01:08,  4.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error creating companies_df_temp for page ?page=85: All arrays must be of the same length\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████████████████████████████████████████████████████████████████████████████████████████████████████▌                  | 85/100 [05:28<00:55,  3.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error creating companies_df_temp for page ?page=86: All arrays must be of the same length\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|█████████████████████████████████████████████████████████████████████████████████████████████████████████▊                 | 86/100 [05:33<00:54,  3.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error creating companies_df_temp for page ?page=87: All arrays must be of the same length\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|███████████████████████████████████████████████████████████████████████████████████████████████████████████                | 87/100 [05:37<00:53,  4.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error creating companies_df_temp for page ?page=88: All arrays must be of the same length\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▏              | 88/100 [05:42<00:50,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error creating companies_df_temp for page ?page=89: All arrays must be of the same length\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▍             | 89/100 [05:47<00:48,  4.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error creating companies_df_temp for page ?page=90: All arrays must be of the same length\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▋            | 90/100 [05:51<00:44,  4.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error creating companies_df_temp for page ?page=91: All arrays must be of the same length\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████▉           | 91/100 [05:55<00:37,  4.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error creating companies_df_temp for page ?page=92: All arrays must be of the same length\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏         | 92/100 [05:58<00:30,  3.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error creating companies_df_temp for page ?page=93: All arrays must be of the same length\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍        | 93/100 [06:01<00:25,  3.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error creating companies_df_temp for page ?page=94: All arrays must be of the same length\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌       | 94/100 [06:06<00:23,  4.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error creating companies_df_temp for page ?page=95: All arrays must be of the same length\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊      | 95/100 [06:10<00:19,  3.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error creating companies_df_temp for page ?page=96: All arrays must be of the same length\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████     | 96/100 [06:13<00:15,  3.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to fetch response_companies page. Status code: 404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎   | 97/100 [06:17<00:11,  3.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to fetch response_companies page. Status code: 404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌  | 98/100 [06:21<00:07,  3.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to fetch response_companies page. Status code: 404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊ | 99/100 [06:25<00:03,  3.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to fetch response_companies page. Status code: 404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [06:29<00:00,  3.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company data collected.\n",
      "Collecting review data for shipping_logistics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# Run scraper for all categories\n",
    "category_suffix_list = [\n",
    "    #\"money_insurance\",\n",
    "    #\"travel_vacation\",\n",
    "    #\"food_beverages_tobacco\",\n",
    "    #\"restaurants_bars\",\n",
    "    #\"events_entertainment\",\n",
    "    #\"beauty_wellbeing\",\n",
    "    #\"shopping_fashion\",\n",
    "    #\"home_garden\",\n",
    "    #\"vehicles_transportation\",\n",
    "    #\"electronics_technology\",\n",
    "    #\"animals_pets\",\n",
    "    \"shipping_logistics\"\n",
    "]\n",
    "\n",
    "for category_suffix in category_suffix_list:\n",
    "    companies_df_full = scrape_trustpilot(\n",
    "        trustpilot_base_url=trustpilot_base_url, \n",
    "        categories_base_url=categories_base_url, \n",
    "        category_suffix=category_suffix,\n",
    "        num_categories_pages=100, \n",
    "        args_url_query=args_url_query,\n",
    "        max_score=3.0,\n",
    "        min_num_reviews=1000,\n",
    "        max_num_review_pages=50\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37c4e5a-c119-4fa4-ab50-4beff53e54a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Add more try/excepts\n",
    "# shipping and logistics not scraping - suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2352fe78-5d70-4af1-bf91-e8b7e7ae4230",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d3cbac-6e9d-44f2-b7db-66aaa597d1db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a081eefa-680b-480b-9e11-30cd97c6925f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24ebcf6-5c36-48cd-ab35-2c36a49ed49d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a87412e-e2b3-4158-9894-3fca7c102831",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0707272-6352-419f-9868-d47e8ff19a22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b996f6-c533-43c4-80f9-2c2d0c39d25e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464e4ec7-39cd-4aba-a6b2-4bcfd46ae12d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4d62fb-1318-4e60-a1af-7d4de6a2ebb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2203f3be-dd54-4a31-9bdb-263570664f62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "davies-hickman-trustpilot-scraper",
   "language": "python",
   "name": "davies-hickman-trustpilot-scraper"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
